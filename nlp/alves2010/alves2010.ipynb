{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 kalimat dengan PartOfSpeech TAG\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# sent singkatan dari sentences\n",
    "def ie_preprocess(document):\n",
    "#     1. segmenter [1],\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "# 2. word tokenizer [2]\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "# 3. part-of-speech tagger [3]:\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]    \n",
    "    return sentences\n",
    "# sisa tahap 4 dan 5\n",
    "alvesSentences = ie_preprocess(open('../papers/alves2010.txt').read())\n",
    "print(len(alvesSentences), 'kalimat dengan PartOfSpeech TAG')\n",
    "# writing file nya di tulis_paper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# koding buat NAMED ENTITY RECONITION ?\n",
    "# NLTK dilengkapi classifier utk recognize named-entity\n",
    "# nltk.ne_chunk(sent, binary=True)\n",
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "type(nltk.ne_chunk(sent))\n",
    "# dicetak di tree.txt saja\n",
    "# If we set the parameter binary=True [1], then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"tuple\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-389359fa07b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mcomparatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparatives\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcomparatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgetComparative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malvesSentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# adjective = JJR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# adverb = RBR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-389359fa07b3>\u001b[0m in \u001b[0;36mgetComparative\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midxword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'JJR'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RBR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                 \u001b[0mcomparatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparatives\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcomparatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgetComparative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malvesSentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"tuple\") to list"
     ]
    }
   ],
   "source": [
    "def getComparative(article):        \n",
    "    comparatives = []\n",
    "    for idxsen, sentences in enumerate(article):                \n",
    "        for idxword, word in enumerate(sentences):\n",
    "            if(word[1] == 'JJR' or word[1] == 'RBR'):\n",
    "                comparatives = comparatives + word\n",
    "    return comparatives\n",
    "getComparative(alvesSentences)\n",
    "# adjective = JJR\n",
    "# adverb = RBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideComparative():\n",
    "\tgetComparative(alvesSentences)    \n",
    "\t# bagus nih,buat membandingkan a > b atau a < b\n",
    "\t# 1. search tuple dengan value JJR OR RBR\n",
    "\t# 2. \t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alvesSentences[1]\n",
    "nltk.help.upenn_tagset()\n",
    "# nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following example searches for strings that contain the word in. The special regular expression (?!\\b.+ing\\b) is a negative lookahead assertion that allows us to disregard strings such as success in supervising the transition of, where in is followed by a gerund.\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = '''\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "of IN B-PP\n",
    "vice NN B-NP\n",
    "chairman NN I-NP\n",
    "of IN B-PP\n",
    "Carlyle NNP B-NP\n",
    "Group NNP I-NP\n",
    ", , O\n",
    "a DT B-NP\n",
    "merchant NN I-NP\n",
    "banking NN I-NP\n",
    "concern NN I-NP\n",
    ". . O\n",
    "'''\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "# result.draw()\n",
    "# (S\n",
    "#  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
    "#  barked/VBD\n",
    "#  at/IN\n",
    "#  (NP the/DT cat/NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToText(paper):\n",
    "    # raw/str -> token/list -> convert ke nltk.Text    \n",
    "    raw = paper.read()    \n",
    "    # type(raw) == string    \n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    # type(tokens) == list    \n",
    "    # token bisa berupa tanda-baca{?.,etc}, pos = {adverb,adj,}    \n",
    "    return nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A concordance view shows us every occurrence of a given word, together with some context. \n",
    "alvesText = convertToText(open('papers/alves2010.txt'))\n",
    "alvesText.concordance('threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dibedakan dengan threshold vs thresholds = noun vs noun+s\n",
    "alvesText.concordance('thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:\n",
    "# noun vs noun+s wkwkwk \n",
    "## gimana cara deteksi phrases/frasa? apakah masalah besar? tidak apa-apa\n",
    "# box = box plot\n",
    "# chidamber  = chidamber kemerer\n",
    "# similar? = Distributional similarity: find other words which appear in the same contexts as the specified word; list most similar words first.\n",
    "alvesText.similar('threshold')\n",
    "print('')\n",
    "print('pakai \"s\" jadi jamak')\n",
    "alvesText.similar('thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]    \n",
    "    return sentences\n",
    "\n",
    "alvesSentences = ie_preprocess(open('papers/alves2010.txt').read())\n",
    "len(alvesSentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
